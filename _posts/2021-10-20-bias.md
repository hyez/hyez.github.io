---
title:  "[ë¦¬ë·°] Algorithmic Bias and Fairness"
categories:
  - Review
tags:
  - Blog
---


## ğŸ’¡ Biasë€ ë¬´ì—‡ì¼ê¹Œ?

<img src='/assets/posts/ìŠ¤í¬ë¦°ìƒ·_2021-10-20_ì˜¤í›„_2.08.18.png' width=300>
<img src='/assets/posts/ìŠ¤í¬ë¦°ìƒ·_2021-10-20_ì˜¤í›„_2.10.42.png' width=300>

ì™¼ìª½ ì‚¬ì§„ì„ ë³´ê³  ì´ ì‚¬ì§„ì´ ë¬´ì—‡ì´ëƒê³  ë¬¼ìœ¼ë©´, ëŒ€ë¶€ë¶„ì˜ ì‚¬ëŒë“¤ì€ `watermelon` ì´ë¼ê³  ëŒ€ë‹µí•  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì˜¤ë¥¸ìª½ ì‚¬ì§„ì„ ë³´ê³  ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µìœ¼ë¡œ `yellow watermelon` ì´ë¼ê³  ëŒ€ë‹µí•  ê²ƒì´ë‹¤. ì™œ ì²«ë²ˆì§¸ ì‚¬ì§„ì„ ë³´ê³  `red watermelon`ì´ë¼ê³  ë§í•˜ì§€ ì•Šì„ê¹Œ? 
ìš°ë¦¬ëŠ” ìˆ˜ë°•ì˜ ìƒ‰ì„ ë‹¹ì—°í•˜ê²Œ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ìƒê°í•œë‹¤. (ì¦‰, `red`ëŠ” ìˆ˜ë°•ì˜ `prototypical color`ì´ë‹¤.) ë”°ë¼ì„œ ì²«ë²ˆì§¸ ì‚¬ì§„ì— êµ³ì´ `red`ë¥¼ ë¶™ì´ì§€ ì•ŠëŠ” ê²ƒì´ë‹¤. 



## ğŸ’¡ ìš©ì–´ ì •ë¦¬
bias, stereotypeì€ **human or AI system**ì— ì˜í•´ ë°œìƒí•œë‹¤.

1. `Prototype` : typical representation of a concept or object (e.g. ìœ„ì˜ ì˜ˆì‹œì—ì„œ ìˆ˜ë°•ì˜ ìƒ‰ì„ redë¼ê³  ìƒê°í•˜ëŠ” ê²ƒ)
2. `Bias` : ë°ì´í„° ë‚´ì— ìˆëŠ” ëª¨ë“  ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠìŒìœ¼ë¡œ ì¸í•´, ì§€ì†ì ìœ¼ë¡œ ì˜ëª»ëœ ê²ƒë“¤ì„ í•™ìŠµí•˜ëŠ” ê²½í–¥
3. `Stereotype` 

## ğŸ’¡ Bias ì¢…ë¥˜
BiasëŠ” í¬ê²Œ `data-driven`ê³¼ `interpretation-driven`ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. 


1. **Data-Driven**
    - `Selection Bias` : data selection ì‹œ ë°œìƒí•˜ëŠ” bias (e.g. class imbalacne)
    - `Reporting Bias` : real likelihoodë¥¼ ë‹¤ ë°˜ì˜í•  ìˆ˜ ì—†ìŒ (e.g. news coverage)
    - `Sampling Bias` : íŠ¹ì • dataê°€ ë” ë§ì´ ìƒ˜í”Œë§ë˜ëŠ” ê²½ìš° (e.g. hair, skin tone)
2. **Interpretation-Driven**
    - `Correlation Fallacy` : correlation â‰  causation
    - `Over-generalization` : "general"ì„ í‰ê°€í•˜ê¸°ìœ„í•´ limited test dataë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë°œìƒ
    - `Automation Bias` : human-generated ë³´ë‹¤ AI-generatedë¥¼ ì„ í˜¸í•˜ê³  human-generatedë¥¼ ë¬´ì‹œí•˜ëŠ” ê²½í–¥

## ğŸ’¡ Fairness ì •ì˜
```markdown
Fairnessë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•ì€ ë‹¤ì–‘í•˜ì§€ë§Œ, ë³¸ ê°•ì˜ì—ì„œëŠ” `Equal Opportunity`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì •ì˜ë¥¼ ì„¤ëª…í•œë‹¤.
ì´ ì™¸ì—ë„ `Equalized Odds`, `Demographic Parity` ë“±ì˜ Fairness ì •ì˜ê°€ ì¡´ì¬í•œë‹¤. 
```

$f_{\theta}(x)$ê°€ classifer ì¼ë•Œ, `sensitive feature` $z$ ì— ë”°ë¼ì„œ ê²°ê³¼ê°€ ë°”ë€Œë©´ $f_{\theta}(x)$ëŠ” biased!


ë°˜ëŒ€ë¡œ, ì•„ë˜ ìˆ˜ì‹ì„ ë§Œì¡±í•˜ë©´ $f$ëŠ” $z$ ì— ê´€í•´ fairí•˜ë‹¤ê³  ë§í•  ìˆ˜ ìˆë‹¤. 
$$f_{\theta}(x) = f_{\theta}(x, z)$$

ì•„ë˜ëŠ” $z$ ê°€ single variable ì¼ë•Œ, fairness ì •ì˜ë¥¼ ë³´ì—¬ì£¼ê³  ìˆë‹¤. ($z$ì˜ ê°’ì´ ë°”ë€Œë”ë¼ë„, ê²°ê³¼ëŠ” ë™ì¼í•œ ê²ƒì„ ì˜ë¯¸í•¨)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-10-20 á„‹á…©á„’á…® 3.32.21.png](/assets/posts/ìŠ¤í¬ë¦°ìƒ·_2021-10-20_ì˜¤í›„_3.32.21.png)

## ğŸ’¡ Tehchniques to Improve Fairness
```markdown
Fairness ê°œì„ ì„ ìœ„í•´ ë¬¸ì œê°€ ë˜ëŠ” featureë¥¼ ì œê±°í•˜ê±°ë‚˜ ìƒˆë¡œìš´ featureë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.
```

1. **Bias Mitigation (Remove)**
    - Biased model, dataset, .. 
    â†’ **Remove** problematic signal 
    â†’ Mitigated bias & Improved fairness
2. **Inclusion (Add)**
    - Biased model, dataset, .. 
    â†’ **Add** signal for desired features 
    â†’ Re-weighted signal & Improved fairness

## ğŸ’¡ Bias Mitigation

1. **Adversarial multi-task learning** 
    - jointly predict output $y$(class label) and $z$(sensitive attribute)
    
    ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-10-20 á„‹á…©á„’á…® 4.08.15.png](/assets/posts/ìŠ¤í¬ë¦°ìƒ·_2021-10-20_ì˜¤í›„_4.08.15.png)
    
2. **Learned latnet structure**
    - Learn latent structure â†’ Estmiate distribution â†’ Adaptively resample data â†’ Learn from fair data distribution
        
        ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-10-20 á„‹á…©á„’á…® 4.16.10.png](/assets/posts/ìŠ¤í¬ë¦°ìƒ·_2021-10-20_ì˜¤í›„_4.16.10.png)
        
    

---

## References

- [https://www.youtube.com/watch?v=wmyVODy_WD8](https://www.youtube.com/watch?v=wmyVODy_WD8)