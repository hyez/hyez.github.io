---
title:  "[리뷰] Counteracting Bias and Increasing Fairness in Search and Recommender Systems"
toc: true
categories:
  - Review
tags:
  - Blog
---

> Recsys 2020 tutorial \<Counteracting Bias and Increasing Fairness in Search and Recommender Systems\>을 듣고 정리한 내용입니다.

## Bias in Rankings

1. **Query "CEO"**
    - 검색 결과는 대부분 `white` & `male` 인 것을 볼 수 있다.
    
    ![스크린샷 2021-10-20 오후 7.34.25.png](/assets/posts/스크린샷_2021-10-20_오후_7.32.25.png)
    
2. **Query "why are black women so"**
    - 검색 결과는 `angry`, `loud`, ..으로 편향된 것을 볼 수 있다.
    
    ![스크린샷 2021-10-20 오후 7.32.25.png](/assets/posts/스크린샷_2021-10-20_오후_7.34.25.png)
    

### Why is this Problematic?

![스크린샷 2021-10-20 오후 7.39.19.png](/assets/posts/스크린샷_2021-10-20_오후_7.39.19.png)

> 위 그래프는 ranking position에 따른 CTR을 보여주고 있다. top position에 위치할수록 유저가 더 많이 클릭을 한다는 것인데, 높은 순위의 아이템의 CTR은 계속해서 높아질 것이고, 낮은 순위의 아이템의 CTR은 계속해서 낮아지는 현상이 생긴다.
> 

## What is Bias?

> 추천시스템에서 bias는 크게 `data bias`, `algorithmic bias`, `presentation bias`, `response bias`로 나뉜다.
> 

![스크린샷 2021-10-20 오후 7.49.39.png](/assets/posts/스크린샷_2021-10-20_오후_7.49.39.png)

### 1. Data Bias

- Cognitive bias
- Differenct group sizes
- Historical, cultural, educational, political reasons
- Sampling bias : 특정 data가 더 많이 샘플링되는 경우 (e.g. hair, skin tone)

### 2. Algorithmic Bias

- 모델은 real world에서 샘플링된 데이터로만 학습하기때문에 bias가 생긴다.
- feature encoding, selection, training, evalaution 과정에서 생기는 bias

### 3. Presentation & Response Bias

- result를 어떻게 보여줄 것인지, 랭킹을 어떻게 하는지에 따라서 생기는 bias

---

## What is Fair?

### 💡 용어 정리

- `Bias`
    - **skewed expousre**를 말한다.
    - e.g. "CEO" 검색 결과 top result는 대부분 male & white인 경우
- `Diversity`
    - **다양한 결과**를 보여주는 것을 말한다. (exposure of **multi-ascpects**)
    - e.g. "CEO" 검색 결과 다양한 genders, races, occupations를 보여주는 경우
- `Novelty`
    - 중복된 결과를 보여주지 않는 것을 말한다. (**reduce** **redundancy**)
- `Fairness`
    - **pre-defined fairness**에 따라서 검색 결과를 보여주는 것이다.
    - 이때 fairness를 정의하는 방법은 다양하게 존재한다.

### Fairness Definitions

Group fairness는 individulal fairness를 보장하지 않지만, individual fairness는 group fairness를 보장한다. 

1. Individual fairness
    - Treat similar individuals similarly
    - e.g. '여성'이라는 그룹 특성과는 관계 X
2. Group fairness
    - 관련된 그룹끼리 다루는 것
    - e.g. '여성'에게는 비슷하게 추천하는 경우

### Statistical Parity

- **Demographic Parity**
$$P(d=1|a) = P(d=1)$$
    - decision d는 sensitive attribute a에 독립적이면 → fairness 
    - e.g. male or female 이랑은 관계 X
- **Equalized Odd**
$$P(d=1|y=y, a) = P(d=1|y=y)$, y= {0, 1}$$
    - original label이 같을 때, decision d가 sensitive attribute a에 독립적이면 → fairness
    - e.g. original label이 같다면, male/female은 관계 X

### Group Fairnes

- **Demographic Parity**
    - `group size proportion`을 result에 반영하는 것
    - e.g. 80% male, 20% female CEO라면 result에 80:20 반영
- **Disparate Treatment**
    - `group utility`를 result에 반영하는것 (utility는 MAE, NDCG 같은 metric을 의미)
- **Disparate Impact**
    - `expected group attention`(=outcome)을 result에 반영하는 것

---

## Fairness-aware Strategies

![스크린샷 2021-10-21 오후 2.53.54.png](/assets/posts/스크린샷_2021-10-21_오후_2.53.54.png)

### 1. Pre-processing

- **Remove bias** in data
    - e.g. sampling, balancing data, repairing data(re-labeling, remove disparate impact)

### 2. In-processing

- encode fairness **as part of the objective function**
    - e.g. regularizer

### 3. Post-processing

- **Fair presentation** of results
    - e.g. re-raniking, greedy approach, constraint, MAB

## Metrics for Fairness-aware Systems

### Utility Metrics

- user utility
- Decision support & accuarcy
    - MAE, precision, recall, F1, hit ratio
- Rank-biased
    - MRR, nDCG
- Diversity, novelty, intent-aware
    - α-nDCG, NRBP
    - ERR-IA, nDCG-IA

### Fairness Metrics

- Distribution- & proportion-based
    - NDKL
    - minSkew, maxSkiew (measure degree of bias)
- Error-based
- Fairness for probabilistic models

## Fairness Optimization

- utility와 fairness를 동시에 optimize

![스크린샷 2021-10-21 오후 3.56.39.png](/assets/posts/스크린샷_2021-10-21_오후_3.56.39.png)

### Optimize Utility

- **[Given]** fairness constraint
- **[Goal]** maximize utility
    - e.g. maximize precision

### Optimize Fairness

- **[Given]** minimum utility threshold
- **[Goal]** maximize fairness
    - e.g. maximize entropy

### Joint Optimization

- **[Given]** trade-off between fairness and utility
- **[Goal]** maximize fairness and utility based on `trade-off`
    - e.g. maximize $f = w_1R + w_2E$
    

---

## References

- [https://www.youtube.com/watch?v=TtF6exuBbSU](https://www.youtube.com/watch?v=TtF6exuBbSU)